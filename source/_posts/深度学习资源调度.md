---
title: 分布式深度学习资源调度
date: 2018-09-14 15:48:39
tags: [deep learnig, scheduling]
mathjax: true
---

# 分布式机器学习相关会议
- 各大机器学习会议上（NIPS， ICML，ICLR）
- 各大系统会议上（SOSP，OSDI，ATC，EuroSys，SoCC）
- 应用对应的顶级会议上（CVPR，KDD）

## 常用的深度学习benchmark
- ImageNet：15 million 带标签的高分辨率224*224的RGB图像，共22,000catagories，1.2TB
- MNIST：输入是28*28是的二值图，输出是0-9这是个数字，60,000 训练图像，10,000测试图像
- Cifar-10：由60,000张32*32的RGB彩色图片构成，共10个分类。50,000张训练，10,000张测试（交叉验证）

## 现有的一些分布式深度学习平台
- 腾讯的Mariana
- Google的DistBelif
- 微软的Adam

<!--more-->

## 分布式机器学习的研究领域

### 从统计、优化理论、优化算法角度来做
关注以下问题：通过分布式并行或其他方法加速训练后，这个新算法还能保证收敛到之前相同的最优值（或者一个满意的最优值）吗？在分布式环境下，收敛能有多快，比非分布式训练快多少？收敛的有多接近，和单机上跑出来的最优解一样吗？收敛需要什么假设？*应该怎么设计训练过程*（比如，怎么抽样数据、怎么更新参数）从而保证能接近某个最优解，同时还保证加速？

### 从机器学习的模型角度
修改原有的模型；提出新的模型；

### 优化方向
- 异步训练：参数服务器架构；减小通信开销（计算开销<<通信开销）
- 数据并行：将数据分成很多小块，加快训练模型；要求数据之间没有依赖性
- 模型并行：如何划分模型？模型之间的关联性要考虑
Petuum和Tensorflow既支持数据并行也支持模型并行
- 对反向传播算法的重调度：使计算时间和通信时间尽可能重叠
- 模型更新：对于全连接层，反向传播的时候，下一层计算的$\Delta w$

### 并行收敛算法满足的假设
- 训练程序的计算任务集中在参数更新函数上
- 每个iteration，数据之间没有依赖性
- 每个iteration开始之前，每个计算节点需要比较容易的拿到模型参数；每个节点计算完成之后，需要比较容易的将梯度收集起来应用于参数更新:
$ \theta(t+1) = \theta(t) - \alpha * \Delta(\theta(t),D_p(t)) $
前两个条件容易满足，但第三个条件在多机环境下涉及网络通讯，如何保证参数共享以及梯度同步成了保证第三个条件要解决的核心问题。

### 神经网络
- 多层神经网络需要非线性映射。如果全连接层没有非线性部分，只有线性部分，那么计算之后，线性的多层神经网络其实可以转换成一层的神经网络。故加入非线性层，多层神经网络才有意义。
- 为防止使用sigmoid激活函数可能导致的梯度消失问题，激活函数一般取relu函数

### 提高验证准确性的方法
- 增加数据集：Adding more data augmentations often reduces the gap between training and validation accuracy. Data augmentation could be reduced in epochs closer to the end.
- 初始学习率设置大一点：Start with a large learning rate and keep it large for a long time. For example, in CIFAR10, you could keep the learning rate at 0.1 for the first 200 epochs and then reduce it to 0.01.
- batch size不能太大：Do not use a batch size that is too large, especially batch size >> number of classes.
  - batch\_size增大，会使训练速度加快，不过太大会导致learning rate不好调，因为lr和batch size之间不是线性关系
  - batch\_size太小，模型训练的慢

### Data Augmentation
人工增加训练集的大小，包括反射(image reflection)、平移(translation)、旋转(rotation)、加噪声等方法从已有的数据中创造一批新的数据

### Batch Normalization
CNN网络在训练的过程中，前一层的参数变化影响着后面层的变化（因为前面层的输出是后面的输入），而且这种影响会随着网络深度的增加而不断放大。在CNN训练时，绝大多数都采用mini-batch使用随机梯度下降算法进行训练，那么随着输入数据的不断变化，以及网络中参数不断调整，网络的各层输入数据的分布则会不断变化，那么各层在训练的过程中就需要不断的改变以适应这种新的数据分布，从而造成网络训练困难，难以拟合的问题。 

BatchNorm的目的是将每一层的输入数据进行归一化(normalization)，使得每一层的数据分布是稳定的--均值0方差1，增加两个可学习的参数 β 和 γ ，对数据进行缩放和平移，从而达到加速训练的目的

### momentum
用来加速训练过程的。引入momentum后，采用如下公式：
```
    v = mu * v - learning_rate * dw
    w = w + v
```
v初始为0，mu是一个超参数，一般设置为0.9。这样理解：如果上一次v和这一次的负梯度方向是相同的，则w下降的幅度会加大，从而使收敛的速度加快；反之，w下降的幅度会减小，收敛的速度减慢。

### Softmax函数
即归一化指数函数，Softmax函数将向量等比例压缩到[0,1]之间，且保证所有元素之和为1。在多分类问题中用作输出层。
$$ softmax(i) = \frac{e^i}{\sum_{j}e^j} $$

向量里面的每个元素都可能取到，只是取到的概率由softmax函数求出的值给出。

在做反向传播的时候，也很方便，只需将softmax算出来的类别向量对应的真正结果的那一维减一就可以了。比如通过若干层的计算，最后得到的某个训练样本的向量的分数是[ 1, 5, 3 ],那softmax计算出来的概率就是$[\frac{e^1}{e^1 + e^3 + e^5},\frac{e^5}{e^1 + e^3 + e^5},\frac{e^3}{e^1 + e^3 + e^5}]=[ 0.015, 0.866, 0.117 ]$，如果这个样本正确的分类是第二个的话，那么计算出来的偏导就是$[ 0.015, 0.866 - 1, 0.117 ] = [ 0.015, -0.134, 0.117 ]$，然后根据这个偏导做反向传播，更新参数值。

## 参考文献
[张昊的知乎专栏](https://zhuanlan.zhihu.com/p/30976469)
[深度学习--Batch Normalization 算法介绍](https://blog.csdn.net/lhanchao/article/details/70308092)
[Softmax的理解和应用](https://blog.csdn.net/superCally/article/details/54234115)