---
title: 分布式深度学习系统
date: 2018-08-01 15:48:39
tags: [deep learnig]
---


## 分布式深度学习系统

- 模型(model)、参数(parameter)、训练数据(data)都需要分布式并行处理
- 迭代次数=数据集大小/batchsize，也即batch的数目；每轮迭代前，worker会从ps pull最新的参数
- 一轮迭代过后，即训练完一个batch后，做反向传播、参数更新

### 《More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server》（NIPS'13）
本文基于Parameter Server模型提出SSP(Stale Synchronous Parallel)模型，解决异步训练中会出现同步问题。主要思路是在每个worker本地建立一个cache，存储参数，worker从本地cache读取参数而不是向ps请求参数，这样减缓了worker等待参数更新的时间。而本地cache的参数与ps的同步则单独处理。

SSP在同步和异步之间进行了折中，以兼顾quality和quantity。即每个worker在迭代的时候都有一个clock counter，记录当前迭代到哪一步了，给定一个阈值s，SSP满足以下规则：
- 最快worker和最慢worker之间相差的迭代步不能超过s，否则最快的迭代步将强制等待最慢的worker。
- 一个clock为c的worker提交一个更新的时候，这个更新的时间戳就是c。
- 当一个clock为c的worker从cache读取一个更新的时候，它读取的参数更新最坏也是`c-s-1`时间戳之前所有的参数。
- 一个worker读取的更新总会比自己产生的所有结果更新。

当s=0时，就是完全同步；当s等于无穷大时，就是完全异步。

<!--more-->

### 《Project Adam: Building an Efficient and Scalable Deep Learning Training System》（OSDI‘14）
这是微软研究院的一分工作，主要是对深度神经网络训练图像识别任务的底层进行了优化，包括以下几点：
- 数据预处理：提前完成图像的转换（包括平移、反射、旋转），将图像加载进内存。这样能减少数据预处理的时间
- 模型训练：
  - 多线程不加锁训练：因为DNN的池化层的带来的弹性性能，是的DNN可以忍受小的噪声，所以一些陈旧的权值(weight)不会影响最终的收敛
  - 处理数据本地性问题：实现了Windows Socket API，相当于一个全局内存地址，减少内存的拷贝，每次只需要给出一个数据的指针即可定位不同机器上的数据的位置
  - 优化浮点运算
  - 降低滞后机器的影响：为避免速度较快的机器上出现线程等待滞后机器上的数据，允许线程并行处理多个image；在判断训练epoch是否结束的时候，判断如果75%的模型副本（即workers）完成处理了就当作一个训练epoch结束了。
  - 减少worker与ps之间传递的参数的数目：对于全连接层，worker直接将激活函数和误差项发送给ps，让ps进行参数的更新。假设M是上一层神经元的个数，N是下一层神经元的个数，则最终worker和ps之间传递的参数的数目从M\*N降到了k\*(M+N)
对全局的参数服务器也进行了优化

实验结果显示Adam异步训练MNIST TOP-1准确率提升了0.24%，这是一个巨大的提升，并且推翻了在卷积领域公认的知识--异步会导致模型预测的准确度降低。同时，ImageNet 22K训练集上，使用4个image server，48个训练机器（分为16个模型副本，每个副本包含3台机器），和10个ps，共62台机器。较之前最好的结果（使用2000台机器训练一周得到Top-1准确率为13.6%），Adam 的Top-1准确率一天就超过了13.6%，训练完用时10天，准确率达到29.8%。

很多技术值得借鉴，像数据预处理、减少内存拷贝、减少worker和ps之间传递的参数的数目等，这些技术都能很好的减少模型收敛的时间。

### 并行计算
SSP模型是基于BSP(Bulk Synchronous Parallel)模型-整体同步模型的一个改进，是一个分布式存储的MIMD（多指令流多数据流）模型。BSP模型是每次迭代之后一个大同步，而SSP模型更像是一个流水线模型。

### 《STRADS: A Distributed Framework for Scheduled Model Parallel Machine Learning》（EuroSys'16）
提出了一种调度的模型并行化(Scheduled Model Parallelism)，即在将模型切分到不同的节点上之前，考虑模型之间的依赖（不考虑依赖可能导致错误），以及模型收敛的速率，使得模型收敛的速率更快。SchMP允许将模型并行算法分成控制组件和更新组件，控制组件负责依赖检查和设置优先权；更新组件在由控制组件规定的并行调度中执行迭代ML更新。

### 《Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters》（ATC'17）
解决分布式GPU训练中的网络带宽问题

文中提出分布式深度学习其中的两点限制：
- 模型更新时可能会是一个很大的矩阵，在将模型更新发送到ps的时候会占满网络带宽
- 由于DNN的迭代性质，会在训练完一个epoch之后进行参数更新。这样会带来通信的暴增，而其他时间没有通信
本文旨在减小每次更新时矩阵的大小；以及对矩阵进行划分，通过重新调度，使参数更新不发生在同一时刻
- DWBP（分布式无等待反向传播算法）用来做通信的负载均衡
- SACP（structure-aware communication protocol）用来最小化通信负载
  - 每两个worker之间建立p2p连接
  - SFB(sufficient factor broadcast)利用p2p连接在worker之间交换parameters
    - 这一点和Adam里面减少worker与ps之间传递的参数的数目的措施有点类似，不过Adam是worker和ps之间交换
    - 设p个worker，k为batch size，Adam总的通信开销为p\*k\*(M+N)+p\*M\*N
    - SFB的通信开销为$(p-1)^2*k*(M+N)$
  - SACP
同时支持CPU和GPU训练

### 参考文献
[张昊的知乎专栏](https://zhuanlan.zhihu.com/p/30976469)
[http://jcf94.com/2017/12/20/2017-12-20-distributeddl/](http://jcf94.com/2017/12/20/2017-12-20-distributeddl/)