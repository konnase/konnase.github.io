---
title: 数据降维算法
date: 2018-04-12 15:26:51
tags: [pca, nca, lda, data reduction]
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# 如何实现降维
## PCA（主成分分析）
PCA可以把可能具有相关性的高维变量合成线性无关的低维变量，称合成后的低维变量为主成分。主成分可以代替原来的高维变量很好的反映数据的分布，而且减少了无关的变量。

不严格地说，主成分可以看做一组特征向量。通过计算数据集中两两变量之间的协方差得到的协方差距阵，再对协方差矩阵求特征值和特征向量，按照特征值的大小排序得到一组特征向量U，确定主成分的个数m(m<d)，从U中按特征值大小从大到小取m个特征向量，这m个特征向量对应的属性个数即构成了降维后的数据空间维度的大小。

PCA降维的步骤：
- 求样本数据（d维，n个样本）对应的每个变量X的均值，并将所有的样例都减去对应的均值。数据中心化的过程。
- 计算协方差矩阵
- 求协方差矩阵对应的特征值和特征向量。
- 将特征值按照从大到小的顺序排列，选择其中的m的特征值，并将对应的m个特征向量作为列向量组成变换矩阵。
- 用原来的d维的样本数据乘以变换矩阵即得到降维后n x m的矩阵，维度从d维降到m维。

<!--more-->

## LDA（线性判别分析）
先讲一个与降维无关的知识--隐含狄利克雷分布(Latent Dirichlet Allocation)，这是看线性判别分析时无意中看到的。
### 隐含狄利克雷分布
主要思想是根据一篇给定的文档，推测其主题分布。
- 考虑人类是怎么生成一篇文档的
  - 先确定这篇文章的几个主题，然后围绕着这几个主题遣词造句，表达成文。

- 考虑机器应该怎么生成一篇文档
  
  计算机在生成文档前，会先划分主题，生成主题分布；对于每一个主题，又有相应的词分布。
  - 按照先验概率选择一篇文档
  - 从狄利克雷分布（即Dirichlet分布）中取样生成文档的主题分布，换言之，主题分布由超参数为的Dirichlet分布生成
  - 从主题的多项式分布中取样生成文档第j个词的主题
  - 从狄利克雷分布（即Dirichlet分布）中取样生成主题对应的词语分布，换言之，词语分布由参数为的Dirichlet分布生成
  - 从词语的多项式分布中采样最终生成词语
  - 重复以上过程，完成一篇文档

LDA是基于贝叶斯框架，认为主题分布和词分布不是唯一确定的，而是随机变量。这种思想与频率派的思想截然相反，后者认为这两种分布应该是确定的，而样本空间是不确定的。
- 如何从已经产生的文档中反推主题分布
  - 需要估计“主题-词项”矩阵Φ和“文档-主题”矩阵Θ，使用变分-EM算法或者Gibbs采样进行估计。
  - 根据大量已知的文档-词项信息，训练出文档-主题矩阵和主题-词项矩阵
  - 将文档中的词项使用矩阵Φ和矩阵Θ得出文档主题。

### 线性判别分析
利用LDA对数据进行降维操作，考虑到了样本类别，是一种典型的监督降维技术。LDA考虑的重点是使投影后的样本在新的空间上需要有最大的类间距离和最小的类内距离。度量类间距离常用的方法是使用投影后两个类别的均值点之间的距离；度量类内距离常用的方法是使用投影后同一类别之间的数据的方差。用类间距离比上类内距离得：
$$J(w)=\frac{w^T S_b w}{w^T S_w w}$$
最大化J(w)就能同时最大化类间距离和最小化类内距离。其中\\(S_b\\)表示类间散度矩阵，\\(S_w\\)表示类内散度矩阵。使用广义特征值问题求解得：
$$S_w^{-1}S_bw = \lambda w$$
求解\\(S_w^{-1}S_b\\)的特征值和特征向量，然后如同PCA一样将原始数据降到m维。

## NCA（近邻成分分析）
NCA是度量学习中经常使用的方法，通过学习一个度量矩阵M来度量样本集中两两样例之间的距离。一般采用梯度下降的方法来拟合度量矩阵M。当M是一个低秩矩阵的时候，可以通过对M进行特征值分解，找到一组正交基，这一组正交基可以组成一个变换矩阵P，P中正交基的数目为矩阵M的秩r，显然r<d（d为原属性数）。则变换矩阵P可用于降维。

## 三种降维方法的对比
PCA是不考虑样本类别输出的无监督降维技术。它主要考察的是数据在投影到新的样本空间后数据之间的方差最大，即投影后的数据的离散程度最高。而不同于PCA的是，LDA在进行投影的时候还考虑到原始样本的类别，而且LDA还假设某一类别的类条件概率密度函数是均值不同、方差相同的高斯分布。LDA是一种监督降维技术。NCA学习出来的降维矩阵P没有复杂的矩阵运算（计算协方差），也无需对样本空间分布进行特定的假设。

## 参考文献
[1] 周志华. 机器学习 : = Machine learning[M]. 清华大学出版社, 2016.
[2] https://blog.csdn.net/HLBoy_happy/article/details/77146012lda
[3] https://zhuanlan.zhihu.com/p/27899927?group_id=869893271453863936